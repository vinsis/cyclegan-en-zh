{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import model, data, data_im2im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_per_item(tensor):\n",
    "    size = tensor.size()\n",
    "    min_vals, _ = tensor.reshape(size[0], -1).min(1, keepdims=True)\n",
    "    max_vals, _ = tensor.reshape(size[0], -1).max(1, keepdims=True)\n",
    "    den = max_vals - min_vals\n",
    "    \n",
    "    tensor = (tensor - min_vals.unsqueeze(-1).unsqueeze(-1)) / den.unsqueeze(-1).unsqueeze(-1)\n",
    "    return tensor\n",
    "\n",
    "def tensor_to_image(tensorA, tensorB):\n",
    "    tensorA = normalize_per_item(tensorA)\n",
    "    tensorB = normalize_per_item(tensorB)\n",
    "    tensor = torch.cat([tensorA, tensorB], dim=-1).permute(0,2,3,1)\n",
    "    size = tensor.size()\n",
    "    tensor = normalize_per_item(tensor).reshape(size[0] * size[1], *size[2:])\n",
    "    return Image.fromarray(tensor.mul(255).numpy().astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netG_en2zh = model.Generator(3,3)\n",
    "_ = netG_en2zh.load_state_dict(torch.load('weights/netG_en2jp_epoch19.pth', map_location='cpu'))\n",
    "_ = netG_en2zh.eval()\n",
    "\n",
    "netG_zh2en = model.Generator(3,3)\n",
    "_ = netG_zh2en.load_state_dict(torch.load('weights/netG_zh2en_epoch10.pth', map_location='cpu'))\n",
    "_ = netG_zh2en.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(data.dataset, batch_size=52, shuffle=False)\n",
    "x = next(iter(loader))\n",
    "\n",
    "with torch.no_grad():\n",
    "    y = netG_en2zh(x['en'])\n",
    "    z = netG_zh2en(x['zh'])\n",
    "    \n",
    "y.size(), z.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = tensor_to_image(x['en'], y)\n",
    "# image.save('./results/en2zh_weight_1e-1_batch_16_epoch_10_all.jpg')\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = tensor_to_image(x['zh'], z)\n",
    "# image.save('./results/zh2en_weight_1e-1_batch_16_epoch_10_all.jpg')\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netG_A2B = model.Generator(3,3)\n",
    "_ = netG_A2B.load_state_dict(torch.load('weights/netG_en2jp_epoch17.pth', map_location='cpu'))\n",
    "_ = netG_A2B.eval()\n",
    "\n",
    "netG_B2A = model.Generator(3,3)\n",
    "_ = netG_B2A.load_state_dict(torch.load('weights/netG_en2jp_epoch17.pth', map_location='cpu'))\n",
    "_ = netG_B2A.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirA = '_/_/'\n",
    "dataset = data_im2im.ImagePairDataset(dirA, dirB)\n",
    "dataset.SIZE = (200,200)\n",
    "loader = DataLoader(dataset, batch_size=20, shuffle=True)\n",
    "x = next(iter(loader))\n",
    "\n",
    "xA, xB = x['A'], x['B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    y = netG_A2B(xA)\n",
    "    z = netG_B2A(xB)\n",
    "    \n",
    "y.size(), z.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "image = tensor_to_image(xA, y)\n",
    "# image.save('./results/im2im_weight_with_en2zh_weights_faces_2.jpg')\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "image = tensor_to_image(xB, z)\n",
    "# image.save('./results/sticker2real_weight_5e-1_batch_16_epoch_80_2.jpg')\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
